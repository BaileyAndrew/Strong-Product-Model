{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the notebook in which we prototyped the algorithm.  It is left in our repository for completeness, but does not correspond to any of the experiments we use in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Riemannian Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formulas here came from:\n",
    "# \"Conic geometric optimisation on the manifold of positive definite matrices\"\n",
    "# By Sra & Hosseini\n",
    "\n",
    "def pd_gradient(\n",
    "    grad: np.ndarray,\n",
    "    X: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Transform Euclidian gradient `grad` at point `X`\n",
    "    into the Riemannian gradient in the relevant geometry.\n",
    "\n",
    "    This assumes the gradient is symmetric, which it is in our case.\n",
    "\n",
    "    Otherwise, need to change `grad` to `(grad + grad.T) / 2`.\n",
    "    \"\"\"\n",
    "\n",
    "    return X @ grad @ X\n",
    "\n",
    "def pd_retraction(\n",
    "    X: np.ndarray,\n",
    "    grad: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform a retraction step on the manifold.\n",
    "    \"\"\"\n",
    "\n",
    "    return X @ linalg.expm(linalg.inv(X) @ grad)\n",
    "\n",
    "def pd_rectraction_definition(\n",
    "    X: np.ndarray,\n",
    "    grad: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform a retraction step on the manifold, using the definition.\n",
    "    \"\"\"\n",
    "    X_sqrt = linalg.sqrtm(X)\n",
    "    X_sqrt_inv = linalg.inv(X_sqrt)\n",
    "    return X_sqrt @ linalg.expm(X_sqrt_inv @ grad @ X_sqrt_inv) @ X_sqrt\n",
    "\n",
    "def pd_grad_retraction(\n",
    "    X: np.ndarray,\n",
    "    grad: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Transform gradient and then retract in one go\n",
    "\n",
    "    Assumes gradient is symmetric, otherwise must symmetrize\n",
    "    \"\"\"\n",
    "\n",
    "    return X @ linalg.expm(grad @ X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we check if everything works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random positive definite matrix\n",
    "n = 5\n",
    "X = np.random.rand(n, n)\n",
    "S = X @ X.T\n",
    "\n",
    "# Generate random symmetric matrix\n",
    "grad = np.random.rand(n, n)\n",
    "grad = (grad + grad.T) / 2\n",
    "\n",
    "# Transform gradient\n",
    "grad_pd = pd_gradient(grad, S)\n",
    "\n",
    "# Perform retraction\n",
    "S_new = pd_retraction(S, grad_pd)\n",
    "S_new_def = pd_rectraction_definition(S, grad_pd)\n",
    "S_new_grad = pd_grad_retraction(S, grad)\n",
    "\n",
    "assert np.allclose(S_new, S_new_def), \\\n",
    "    \"Uh-oh, retraction methods don't match!\"\n",
    "\n",
    "assert np.allclose(S_new, S_new_grad), \\\n",
    "    \"Uh-oh, retraction methods don't match!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simultaneous Diagonalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_diag(\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray\n",
    ") -> tuple[\n",
    "    np.ndarray,\n",
    "    np.ndarray\n",
    "]:\n",
    "    \"\"\"\n",
    "    Simultaneously diagonalize two positive definite matrices by congruence\n",
    "    (Technically only X must be positive definite, the other must be symmetric)\n",
    "\n",
    "    Returns P, D such that X = PP^T and Y = PDP^T\n",
    "    \"\"\"\n",
    "\n",
    "    X_sqrt = linalg.sqrtm(X)\n",
    "    X_sqrt_inv = linalg.inv(X_sqrt)\n",
    "    S = X_sqrt_inv @ Y @ X_sqrt_inv\n",
    "    D, V = linalg.eigh(S)\n",
    "    P = X_sqrt @ V\n",
    "\n",
    "    return P, D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if this works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two random positive definite matrices\n",
    "n = 5\n",
    "X = np.random.rand(n, 3 * n)\n",
    "Y = np.random.rand(n, 3 * n)\n",
    "X = X @ X.T\n",
    "Y = Y @ Y.T\n",
    "\n",
    "# Simultaneously diagonalize\n",
    "P, D = sim_diag(X, Y)\n",
    "\n",
    "# Check that the matrices are indeed diagonalized\n",
    "assert np.allclose(P @ P.T, X), \\\n",
    "    \"X not diagonalized\"\n",
    "assert np.allclose(P @ np.diag(D) @ P.T, Y), \\\n",
    "    \"Y not diagonalized\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strong Product Model Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blockwise_trace_ks(\n",
    "    Lam: np.ndarray,\n",
    "    D: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes tr_d2[(Lam kronsum D)^-1]\n",
    "\n",
    "    Lam, D are diagonal matrices\n",
    "    \"\"\"\n",
    "\n",
    "    internal = 1 / (Lam[:, None] + D[None, :])\n",
    "    return internal.sum(axis=1)\n",
    "\n",
    "def stridewise_trace_ks(\n",
    "    Lam: np.ndarray,\n",
    "    D: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes tr^d1[(Lam kronsum D)^-1]\n",
    "\n",
    "    Lam, D are diagonal matrices\n",
    "    \"\"\"\n",
    "\n",
    "    internal = 1 / (Lam[:, None] + D[None, :])\n",
    "    return internal.sum(axis=0)\n",
    "\n",
    "def stridewise_trace_mult(\n",
    "    Lam: np.ndarray,\n",
    "    D: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes tr^d1[(Lam kronsum D)^-1 * (Lam kronprod I)]\n",
    "\n",
    "    Lam, D are diagonal matrices\n",
    "    \"\"\"\n",
    "\n",
    "    internal = Lam[:, None] / (Lam[:, None] * D[None, :])\n",
    "    return internal.sum(axis=0)\n",
    "\n",
    "def vec_kron_sum(Xs: list) -> np.array:\n",
    "    \"\"\"Compute the Kronecker vector-sum\"\"\"\n",
    "    if len(Xs) == 1:\n",
    "        return Xs[0]\n",
    "    elif len(Xs) == 2:\n",
    "        return np.kron(Xs[0], np.ones(Xs[1].shape[0])) + np.kron(np.ones(Xs[0].shape[0]), Xs[1])\n",
    "    else:\n",
    "        d_slash0 = np.prod([X.shape[0] for X in Xs[1:]])\n",
    "        return (\n",
    "            np.kron(Xs[0], np.ones(d_slash0))\n",
    "            + np.kron(np.ones(Xs[0].shape[0]), vec_kron_sum(Xs[1:]))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(\n",
    "    Psi_1: np.ndarray,\n",
    "    Theta: np.ndarray,\n",
    "    Psi_2w: np.ndarray,\n",
    "    S_2: np.ndarray,\n",
    "    Data: np.ndarray\n",
    ") -> float:\n",
    "    Lam, _ = linalg.eigh(Psi_1)\n",
    "    _, D = sim_diag(Theta, Psi_2w)\n",
    "    _, detTheta = np.linalg.slogdet(Theta)\n",
    "\n",
    "    if Lam.min() <= 0 or D.min() <= 0:\n",
    "        # Don't allow non-positive-definite matrices\n",
    "        return np.inf\n",
    "\n",
    "    logdets = - np.log(vec_kron_sum([Lam, D])).sum() - Psi_1.shape[0] * detTheta\n",
    "    traces = np.trace(Psi_2w @ S_2) + np.trace(Psi_1 @ Data @ Theta @ Data.T)\n",
    "\n",
    "    return logdets + traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo(\n",
    "    new_value: float,\n",
    "    old_value: float,\n",
    "    eta: float,\n",
    "    beta: float,\n",
    "    grad_norm: tuple[np.ndarray]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Armijo line search\n",
    "    \"\"\"\n",
    "    return new_value <= old_value - eta * beta * grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(\n",
    "    X: np.ndarray,\n",
    "    S_2: np.ndarray,\n",
    "    Psi_1: np.ndarray,\n",
    "    V: np.ndarray,\n",
    "    Lam: np.ndarray,\n",
    "    Theta: np.ndarray,\n",
    "    P: np.ndarray,\n",
    "    Psi_2w: np.ndarray,\n",
    "    D: np.ndarray,\n",
    "    rho_psi_1: float,\n",
    "    rho_psi_2w: float,\n",
    "    rho_theta: float,\n",
    ") -> tuple[\n",
    "    np.ndarray,\n",
    "    np.ndarray,\n",
    "    np.ndarray,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Computes G * Gamma, where G is the Euclidean gradient at Gamma,\n",
    "    and Gamma is each of our three parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    P_inv = linalg.inv(P)\n",
    "\n",
    "    Psi_1_core = blockwise_trace_ks(Lam, D)\n",
    "    Psi_2w_core = stridewise_trace_ks(Lam, D)\n",
    "    Theta_core = stridewise_trace_mult(Lam, D)\n",
    "\n",
    "    XTheta = X @ Theta\n",
    "    XtPsi = X.T @ Psi_1\n",
    "\n",
    "    log_Psi_1 = (V * np.log(Lam)) @ V.T\n",
    "    log_Psi_2w = linalg.logm(Psi_2w)\n",
    "    log_Theta = linalg.logm(Theta)\n",
    "\n",
    "    Psi_1_grad = - (V * Psi_1_core) @ V.T + XTheta @ X.T + 2 * rho_psi_1 * np.linalg.inv(Psi_1) @ log_Psi_1\n",
    "    #assert (np.allclose(- (V * Psi_1_core) @ V.T, (- (V * Psi_1_core) @ V.T).T))\n",
    "    #assert (np.allclose(XTheta @ X.T , (XTheta @ X.T).T))\n",
    "    #assert (np.allclose(2 * rho_psi_1 * np.linalg.inv(Psi_1) @ log_Psi_1 , (2 * rho_psi_1 * np.linalg.inv(Psi_1) @ log_Psi_1).T))\n",
    "    Psi_2w_grad = - (P_inv.T * Psi_2w_core) @ P_inv + S_2 + 2 * rho_psi_2w * np.linalg.inv(Psi_2w) @ log_Psi_2w\n",
    "    Theta_grad = - (P_inv.T * Theta_core) @ P_inv + XtPsi @ X + 2 * rho_theta * np.linalg.inv(Theta) @ log_Theta\n",
    "\n",
    "    Psi_1_grad = (Psi_1_grad + Psi_1_grad.T) / 2\n",
    "    Psi_2w_grad = (Psi_2w_grad + Psi_2w_grad.T) / 2\n",
    "    Theta_grad = (Theta_grad + Theta_grad.T) / 2\n",
    "\n",
    "    return Psi_1_grad, Theta_grad, Psi_2w_grad\n",
    "\n",
    "\n",
    "def gradients_shifted(\n",
    "    X: np.ndarray,\n",
    "    S_2: np.ndarray,\n",
    "    Psi_1: np.ndarray,\n",
    "    V: np.ndarray,\n",
    "    Lam: np.ndarray,\n",
    "    Theta: np.ndarray,\n",
    "    P: np.ndarray,\n",
    "    Psi_2w: np.ndarray,\n",
    "    D: np.ndarray,\n",
    "    rho_psi_1: float,\n",
    "    rho_psi_2w: float,\n",
    "    rho_theta: float,\n",
    ") -> tuple[\n",
    "    np.ndarray,\n",
    "    np.ndarray,\n",
    "    np.ndarray,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Computes G * Gamma, where G is the Euclidean gradient at Gamma,\n",
    "    and Gamma is each of our three parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    P_inv = linalg.inv(P)\n",
    "\n",
    "    Psi_1_core = blockwise_trace_ks(Lam, D) * Lam\n",
    "    Psi_2w_core = stridewise_trace_ks(Lam, D) * D\n",
    "    Theta_core = stridewise_trace_mult(Lam, D)\n",
    "\n",
    "    XTheta = X @ Theta\n",
    "    XtPsi = X.T @ Psi_1\n",
    "\n",
    "    log_Psi_1 = (V * np.log(Lam)) @ V.T\n",
    "    log_Psi_2w = linalg.logm(Psi_2w)\n",
    "    log_Theta = linalg.logm(Theta)\n",
    "\n",
    "    Psi_1_grad = - (V * Psi_1_core) @ V.T + XTheta @ XtPsi + 2 * rho_psi_1 * log_Psi_1\n",
    "    Psi_2w_grad = - (P_inv.T * Psi_2w_core) @ P.T + S_2 @ Psi_2w + 2 * rho_psi_2w * log_Psi_2w\n",
    "    Theta_grad = - (P_inv.T * Theta_core) @ P.T + XtPsi @ XTheta + 2 * rho_theta * log_Theta\n",
    "\n",
    "    return Psi_1_grad, Theta_grad, Psi_2w_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5) (6, 6) (6, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.91995063, 3.56222315, 3.36246415, 3.46802192, 3.32214367],\n",
       "       [4.26044885, 4.07600713, 4.72198365, 4.92783979, 4.82463725],\n",
       "       [3.08362713, 3.61089228, 2.347799  , 3.5378608 , 3.39123785],\n",
       "       [3.56097982, 4.22641289, 3.96495407, 3.12076385, 3.99606477],\n",
       "       [4.00400513, 4.85399415, 4.46478607, 4.68464317, 3.5382007 ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate three random positive definite matrices\n",
    "n = 5\n",
    "samples = 10\n",
    "X = np.random.rand(n, samples * n)\n",
    "Y = np.random.rand(n + 1, samples * n)\n",
    "Z = np.random.rand(n + 1, samples * n)\n",
    "X = X @ X.T / (n * samples)\n",
    "Y = Y @ Y.T / (n * samples)\n",
    "Z = Z @ Z.T / (n * samples)\n",
    "\n",
    "# Eigendecompose the first matrix\n",
    "Lam, V = linalg.eigh(X)\n",
    "\n",
    "# Simultaneously diagonalize the other two\n",
    "P, D = sim_diag(Y, Z)\n",
    "\n",
    "# Generate random data matrix\n",
    "Data = np.random.rand(n, n+1)\n",
    "S_2 = Data.T @ Data\n",
    "\n",
    "# Check that the gradients_shifted function runs\n",
    "A, B, C = gradients_shifted(\n",
    "    Data,\n",
    "    S_2,\n",
    "    X,\n",
    "    V,\n",
    "    Lam,\n",
    "    Y,\n",
    "    P,\n",
    "    Z,\n",
    "    D,\n",
    "    0.1,\n",
    "    0.1,\n",
    "    0.1\n",
    ")\n",
    "print(A.shape, B.shape, C.shape)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35409367 0.30435395 0.27052285 0.29553409 0.2962426 ]\n",
      " [0.30435395 0.44671222 0.34719354 0.36117632 0.33003236]\n",
      " [0.27052285 0.34719354 0.410154   0.32956301 0.32977825]\n",
      " [0.29553409 0.36117632 0.32956301 0.4295315  0.33540936]\n",
      " [0.2962426  0.33003236 0.32977825 0.33540936 0.40088303]]\n",
      "[[0.48460813 0.36847968 0.40350571 0.37347465 0.35519743 0.37829382]\n",
      " [0.36847968 0.4122708  0.3685833  0.3263359  0.3329526  0.34391307]\n",
      " [0.40350571 0.3685833  0.47696326 0.37101104 0.38411322 0.35740317]\n",
      " [0.37347465 0.3263359  0.37101104 0.39794199 0.35675886 0.32969898]\n",
      " [0.35519743 0.3329526  0.38411322 0.35675886 0.40907015 0.33658295]\n",
      " [0.37829382 0.34391307 0.35740317 0.32969898 0.33658295 0.42906397]]\n",
      "[[0.34980305 0.28150628 0.2717028  0.3126787  0.26431591 0.22925258]\n",
      " [0.28150628 0.37233941 0.30506463 0.33195329 0.2623392  0.27647295]\n",
      " [0.2717028  0.30506463 0.38312323 0.33973025 0.28020184 0.28565458]\n",
      " [0.3126787  0.33195329 0.33973025 0.44165262 0.32379263 0.31789422]\n",
      " [0.26431591 0.2623392  0.28020184 0.32379263 0.3580383  0.27882175]\n",
      " [0.22925258 0.27647295 0.28565458 0.31789422 0.27882175 0.33615337]]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "print(X @ linalg.expm(lr * A))\n",
    "print(Y @ linalg.expm(lr * B))\n",
    "print(Z @ linalg.expm(lr * C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4543.588094440915\n",
      "-4715.743394162065\n",
      "-4795.956639188187\n",
      "-4815.388394380641\n",
      "-4816.594078067642\n",
      "-4817.196608608265\n",
      "-4817.271907557166\n",
      "-4817.290731730178\n",
      "-4817.300143740756\n",
      "-4817.302496734592\n",
      "-4817.302496734592\n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "# Generate three random positive definite matrices\n",
    "n = 100\n",
    "samples = n + 2\n",
    "# X = np.random.rand(n, samples * n)\n",
    "# Y = np.random.rand(n + 1, samples * n)\n",
    "# Z = np.random.rand(n + 1, samples * n)\n",
    "# X = X @ X.T# / (n * samples)\n",
    "# Y = Y @ Y.T# / (n * samples)\n",
    "# Z = Z @ Z.T# / (n * samples)\n",
    "X = np.eye(n)\n",
    "Y = np.eye(samples)\n",
    "Z = 2 * np.eye(samples)\n",
    "\n",
    "# Generate random data matrix\n",
    "Data = np.random.rand(n, samples)\n",
    "S_2 = Data.T @ Data\n",
    "\n",
    "for i in range(100):\n",
    "    # Eigendecompose the first matrix\n",
    "    Lam, V = linalg.eigh(X)\n",
    "\n",
    "    # Simultaneously diagonalize the other two\n",
    "    P, D = sim_diag(Y, Z)\n",
    "\n",
    "    # Check that the gradients_shifted function runs\n",
    "    A, B, C = gradients_shifted(\n",
    "        Data,\n",
    "        S_2,\n",
    "        X,\n",
    "        V,\n",
    "        Lam,\n",
    "        Y,\n",
    "        P,\n",
    "        Z,\n",
    "        D,\n",
    "        0.00001,\n",
    "        0.00001,\n",
    "        0.00001\n",
    "    )\n",
    "    # A, B, C = gradients(\n",
    "    #     Data,\n",
    "    #     S_2,\n",
    "    #     X,\n",
    "    #     V,\n",
    "    #     Lam,\n",
    "    #     Y,\n",
    "    #     P,\n",
    "    #     Z,\n",
    "    #     D,\n",
    "    #     1,\n",
    "    #     1,\n",
    "    #     1\n",
    "    # )\n",
    "    # A @= linalg.inv(X)\n",
    "    # B @= linalg.inv(Y)\n",
    "    # C @= linalg.inv(Z)\n",
    "\n",
    "    # print('----')\n",
    "    # test, _ = np.linalg.eigh(X)\n",
    "    # print(test)\n",
    "    # test, _ = np.linalg.eigh(Y)\n",
    "    # print(test)\n",
    "    # test, _ = np.linalg.eigh(Z)\n",
    "    # print(test)\n",
    "    # print('----')\n",
    "\n",
    "    # print(A @ np.linalg.inv(X))\n",
    "    # print(A_)\n",
    "\n",
    "\n",
    "    old_NLL = NLL(X, Y, Z, S_2, Data)\n",
    "    line_search_init = 1\n",
    "    decrease = 0.5\n",
    "    beta = 0.5\n",
    "    lr = line_search_init\n",
    "\n",
    "    old_X = X\n",
    "    old_Y = Y\n",
    "    old_Z = Z\n",
    "\n",
    "    X = old_X @ linalg.expm(-lr * A)\n",
    "    Y = old_Y @ linalg.expm(-lr * B)\n",
    "    Z = old_Z @ linalg.expm(-lr * C)\n",
    "\n",
    "    grad_norm = (\n",
    "        np.trace(np.linalg.matrix_power(A @ np.linalg.inv(old_X), 2))\n",
    "        + np.trace(np.linalg.matrix_power(B @ np.linalg.inv(old_Y), 2))\n",
    "        + np.trace(np.linalg.matrix_power(C @ np.linalg.inv(old_Z), 2))\n",
    "    )\n",
    "\n",
    "    old_value = NLL(old_X, old_Y, old_Z, S_2, Data)\n",
    "\n",
    "    try:\n",
    "        new_value = NLL(X, Y, Z, S_2, Data)\n",
    "    except:\n",
    "        new_value = np.inf\n",
    "\n",
    "    converged = False\n",
    "    while True:\n",
    "        if new_value < old_value - lr * beta * grad_norm:\n",
    "            # Check if all matrices are positive definite\n",
    "            X_posdef = (np.linalg.eigh(X)[0] > 0).all()\n",
    "            Y_posdef = (np.linalg.eigh(Y)[0] > 0).all()\n",
    "            Z_posdef = (np.linalg.eigh(Z)[0] > 1).all()\n",
    "            if X_posdef and Y_posdef and Z_posdef:\n",
    "                break\n",
    "        lr *= decrease\n",
    "        X = old_X @ linalg.expm(-lr * A)\n",
    "        Y = old_Y @ linalg.expm(-lr * B)\n",
    "        Z = old_Z @ linalg.expm(-lr * C)\n",
    "        try:\n",
    "            new_value = NLL(X, Y, Z, S_2, Data)\n",
    "        except:\n",
    "            new_value = np.inf\n",
    "        if lr < 1e-10:\n",
    "            X = old_X\n",
    "            Y = old_Y\n",
    "            Z = old_Z\n",
    "            converged = True\n",
    "            break\n",
    "    \n",
    "    if converged:\n",
    "        print(old_value)\n",
    "        print(\"Converged\")\n",
    "        break\n",
    "    else:\n",
    "        #print(lr)\n",
    "        print(new_value)\n",
    "\n",
    "    #print(X)\n",
    "    #print(Y)\n",
    "    # print(Z)\n",
    "\n",
    "    # Always stays in the positive definite cone\n",
    "    # print((np.linalg.eigh(X)[0] > 0).all())\n",
    "    # print((np.linalg.eigh(Y)[0] > 0).all())\n",
    "    # print((np.linalg.eigh(Z)[0] > 0).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00117788 -0.00248379 -0.0028727  ... -0.00280856 -0.00278121\n",
      "  -0.00254958]\n",
      " [-0.00248379  1.00101376 -0.00270991 ... -0.00274895 -0.00269723\n",
      "  -0.00291966]\n",
      " [-0.0028727  -0.00270991  1.00090429 ... -0.00290925 -0.00282982\n",
      "  -0.00275604]\n",
      " ...\n",
      " [-0.00280856 -0.00274895 -0.00290925 ...  1.00093895 -0.00292706\n",
      "  -0.00289389]\n",
      " [-0.00278121 -0.00269723 -0.00282982 ... -0.00292706  1.00074864\n",
      "  -0.00274208]\n",
      " [-0.00254958 -0.00291966 -0.00275604 ... -0.00289389 -0.00274208\n",
      "   1.00107035]] [[ 1.00350699 -0.00283769 -0.00298779 ... -0.00320679 -0.00312044\n",
      "  -0.00273313]\n",
      " [-0.00283769  1.00305106 -0.00294913 ... -0.00319252 -0.00283918\n",
      "  -0.00272951]\n",
      " [-0.00298779 -0.00294913  1.00269614 ... -0.00371704 -0.00317598\n",
      "  -0.00298648]\n",
      " ...\n",
      " [-0.00320679 -0.00319252 -0.00371704 ...  1.00250227 -0.00330761\n",
      "  -0.00304322]\n",
      " [-0.00312044 -0.00283918 -0.00317598 ... -0.00330761  1.00342546\n",
      "  -0.00278505]\n",
      " [-0.00273313 -0.00272951 -0.00298648 ... -0.00304322 -0.00278505\n",
      "   1.00361626]] [[ 2.00583282 -0.00970988 -0.01015158 ... -0.01096471 -0.01083931\n",
      "  -0.00936115]\n",
      " [-0.00970988  2.00405377 -0.00997066 ... -0.01087993 -0.00968775\n",
      "  -0.00932372]\n",
      " [-0.01015158 -0.00997066  2.0029705  ... -0.01279747 -0.01087497\n",
      "  -0.01019789]\n",
      " ...\n",
      " [-0.01096471 -0.01087993 -0.01279747 ...  2.0023407  -0.01133645\n",
      "  -0.01036257]\n",
      " [-0.01083931 -0.00968775 -0.01087497 ... -0.01133645  2.00556222\n",
      "  -0.009542  ]\n",
      " [-0.00936115 -0.00932372 -0.01019789 ... -0.01036257 -0.009542\n",
      "   2.0061796 ]]\n",
      "1.4213881881907287 1.4204566662022433 2.0197056629615275\n"
     ]
    }
   ],
   "source": [
    "print(X, Y, Z)\n",
    "print(np.linalg.cond(X), np.linalg.cond(Y), np.linalg.cond(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if our `strong-product-model.py` implementation is correct\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from strong_product_model import strong_product_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: -3987.9682313719522\n",
      "Iteration 2: -4656.532210287609\n",
      "Iteration 3: -4788.189373038928\n",
      "Iteration 4: -4819.460535525576\n",
      "Iteration 5: -4823.323119813105\n",
      "Iteration 6: -4823.564181939269\n",
      "Iteration 7: -4823.5943119249605\n",
      "Iteration 8: -4823.609376723326\n",
      "Iteration 9: -4823.616909073877\n",
      "Iteration 10: -4823.617379844452\n",
      "Iteration 11: -4823.617615229698\n",
      "Iteration 12: -4823.617644652852\n",
      "Iteration 13: -4823.617659364429\n",
      "Iteration 14: -4823.617666720207\n",
      "Iteration 15: -4823.617670398108\n",
      "Iteration 16: -4823.617672237058\n",
      "Iteration 17: -4823.617673156536\n",
      "Iteration 18: -4823.617673271465\n",
      "Iteration 19: -4823.6176733289385\n",
      "Iteration 20: -4823.617673357666\n",
      "Iteration 21: -4823.617673372028\n",
      "Iteration 22: -4823.6176733729335\n",
      "Iteration 23: -4823.617673373041\n",
      "Iteration 24: -4823.617673373088 (converged)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rows': array([[ 1.00155615, -0.0024746 , -0.00289182, ..., -0.00282123,\n",
       "         -0.00279646, -0.00254694],\n",
       "        [-0.0024746 ,  1.00139461, -0.00270154, ..., -0.00274495,\n",
       "         -0.00269344, -0.00294576],\n",
       "        [-0.00289182, -0.00270154,  1.00129928, ..., -0.00290883,\n",
       "         -0.00282696, -0.00275234],\n",
       "        ...,\n",
       "        [-0.00282123, -0.00274495, -0.00290883, ...,  1.00133718,\n",
       "         -0.00293454, -0.00290484],\n",
       "        [-0.00279646, -0.00269344, -0.00282696, ..., -0.00293454,\n",
       "          1.00111537, -0.00274269],\n",
       "        [-0.00254694, -0.00294576, -0.00275234, ..., -0.00290484,\n",
       "         -0.00274269,  1.00145735]]),\n",
       " 'cols_within_rows': array([[ 1.00415054, -0.00284068, -0.0029784 , ..., -0.00320875,\n",
       "         -0.00315195, -0.00273758],\n",
       "        [-0.00284068,  1.00365597, -0.00293123, ..., -0.0031882 ,\n",
       "         -0.00283743, -0.0027296 ],\n",
       "        [-0.0029784 , -0.00293123,  1.00332352, ..., -0.00373466,\n",
       "         -0.00318064, -0.00298594],\n",
       "        ...,\n",
       "        [-0.00320875, -0.0031882 , -0.00373466, ...,  1.00313531,\n",
       "         -0.00331434, -0.00303761],\n",
       "        [-0.00315195, -0.00283743, -0.00318064, ..., -0.00331434,\n",
       "          1.00407038, -0.00279011],\n",
       "        [-0.00273758, -0.0027296 , -0.00298594, ..., -0.00303761,\n",
       "         -0.00279011,  1.0042552 ]]),\n",
       " 'cols_between_rows': array([[ 1.0073959 , -0.00973329, -0.01012589, ..., -0.01098556,\n",
       "         -0.01097841, -0.00939006],\n",
       "        [-0.00973329,  1.00545992, -0.00991082, ..., -0.01087549,\n",
       "         -0.00969217, -0.00933517],\n",
       "        [-0.01012589, -0.00991082,  1.00446532, ..., -0.01288307,\n",
       "         -0.01090649, -0.01020774],\n",
       "        ...,\n",
       "        [-0.01098556, -0.01087549, -0.01288307, ...,  1.00385744,\n",
       "         -0.01137678, -0.01035241],\n",
       "        [-0.01097841, -0.00969217, -0.01090649, ..., -0.01137678,\n",
       "          1.00713046, -0.00957353],\n",
       "        [-0.00939006, -0.00933517, -0.01020774, ..., -0.01035241,\n",
       "         -0.00957353,  1.00772465]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n = 500\n",
    "#samples = n + 2\n",
    "#Data = np.random.rand(n, samples)\n",
    "\n",
    "# Using same data from test implementation for comparison\n",
    "\n",
    "strong_product_model(\n",
    "    data_matrix=Data,\n",
    "    rho_rows=0.0001,\n",
    "    rho_cols_within_rows=0.0001,\n",
    "    rho_cols_between_rows=0.0001,\n",
    "    verbose=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnr-colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
